{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b99c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from base_train.py by Andrej Karpathy in reqchat. repository.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Train model. Run as:\n",
    "\n",
    "python base_train.py \n",
    "\n",
    "or distributed as:\n",
    "\n",
    "torchrun --nproc_per_node=8 base_train.py\n",
    "\n",
    "If you are only on CPU/Macbook, you'll want to train a much much smaller LLM. Example:\n",
    "python -m scripts.base_train --depth=4 --max_seq_len=512 --device_batch_size=1 --eval_tokens=512 --core_metric_every=-1 --total_batch_size=512 --num_iterations=20\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01170862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- set working directory to reqchat root ---\n",
    "# Setting the path to your actual reqchat project root\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f142dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "# default config values -- can be overridden by command line args\n",
    "# Added to allow easy running inside Jupyter notebooks on my cpu machine\n",
    "# Adjust them as per your hardware capabilities\n",
    "# These will not be good for providing meaningful training results, but at least the code will run\n",
    "# Then, you can change them back based on your hardware and accuracy needs\n",
    "#---------------------------------------------------------------------------\n",
    "sys.argv = ['--depth=4', '--max_seq_len=64', '--device_batch_size=1', \n",
    "            '--eval_tokens=256', '--core_metric_every=-1', '--total_batch_size=64', \n",
    "            '--num_iterations=20', '--compile=False']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "216cf023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 2 shards using 4 workers...\n",
      "Target directory: C:\\Users\\mohan\\.cache\\reqchat\\base_data\n",
      "\n",
      "Skipping C:\\Users\\mohan\\.cache\\reqchat\\base_data\\shard_00000.parquet (already exists)\n",
      "Skipping C:\\Users\\mohan\\.cache\\reqchat\\base_data\\shard_00001.parquet (already exists)\n",
      "Done! Downloaded: 2/2 shards to C:\\Users\\mohan\\.cache\\reqchat\\base_data\n"
     ]
    }
   ],
   "source": [
    "# To download the dataset, run: the following command\n",
    "# download just 2 shard of data for testing (minimum for the code to run one for training and one for validation)\n",
    "# Adjust the -n parameter to download more shards based on your needs\n",
    "# Again this is just for providing a minimal dataset to allow the code to run\n",
    "# this will be stored in the default location: C:\\Users\\Username\\.reqchat\\base_data\n",
    "!python -m reqchat.dataset -n 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bddaaa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# Most of the modules we need are in reqchat package\n",
    "# If any missing import error occurs, make sure you installed them via pip install missing_package\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "from reqchat.gpt import GPT, GPTConfig\n",
    "from reqchat.dataloader import tokenizing_distributed_data_loader, tokenizing_distributed_data_loader_with_state\n",
    "from reqchat.common import compute_init, compute_cleanup, print0, DummyWandb, print_banner, get_base_dir, autodetect_device_type\n",
    "from reqchat.tokenizer import get_tokenizer, get_token_bytes\n",
    "from reqchat.checkpoint_manager import save_checkpoint, load_checkpoint\n",
    "from reqchat.loss_eval import evaluate_bpb\n",
    "from reqchat.engine import Engine\n",
    "from scripts.base_eval import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "564da85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: max_seq_len = 64\n",
      "Overriding: device_batch_size = 1\n",
      "Overriding: eval_tokens = 256\n",
      "Overriding: core_metric_every = -1\n",
      "Overriding: total_batch_size = 64\n",
      "Overriding: num_iterations = 20\n",
      "Overriding: compile = False\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# User settings\n",
    "run = \"dummy\" # wandb run name default (\"dummy\" is special - we won't log to wandb)\n",
    "# Runtime\n",
    "device_type = \"\" # cuda|cpu|mps (empty => autodetect good device type default, in order: CUDA > MPS > CPU)\n",
    "# Model architecture\n",
    "depth = 20 # the depth of the Transformer model to train, rest of the kwargs are derived\n",
    "max_seq_len = 2048 # max context length\n",
    "# Training horizon. Only one of these 3 will be used, in this order of precedence.\n",
    "num_iterations = -1 # explicit number of steps of the optimization (-1 = disable)\n",
    "target_flops = -1.0 # calculate num_iterations to reach target_flops. Useful for scaling laws experiments (-1 = disable)\n",
    "target_param_data_ratio = 20 # calculate num_iterations to maintain fixed data:param ratio (Chinchilla=20) (-1 = disable)\n",
    "# Optimization\n",
    "device_batch_size = 32 # per-device batch size (set to not OOM)\n",
    "total_batch_size = 524288 # total desired batch size, in #tokens\n",
    "embedding_lr = 0.2 # learning rate for the embedding parameters (Adam)\n",
    "unembedding_lr = 0.004 # learning rate for the unembedding parameters (Adam)\n",
    "weight_decay = 0.0 # weight decay for the embedding/unembedding parameters (Adam)\n",
    "matrix_lr = 0.02 # learning rate for the matrix parameters (Muon)\n",
    "grad_clip = 1.0 # gradient clipping value (0.0 = disabled)\n",
    "warmup_ratio = 0.0 # ratio of iterations for LR warmup\n",
    "warmdown_ratio = 0.2 # ratio of iterations for LR warmdown\n",
    "final_lr_frac = 0.0 # final LR is this fraction of the initial LR\n",
    "resume_from_step = -1 # resume training from this step of the optimization (-1 = disable)\n",
    "# Evaluation\n",
    "eval_every = 250 # every how many steps to evaluate the model for val bpb\n",
    "eval_tokens = 20*524288 # number of tokens to evaluate val loss on\n",
    "core_metric_every = 2000 # every how many steps to evaluate the core metric (-1 = disable)\n",
    "core_metric_max_per_task = 500 # examples per task in estimating the core metric\n",
    "sample_every = 2000 # every how many steps to sample from the model\n",
    "save_every = -1 # every how many steps to save model checkpoints (-1 = disable, and save only at the end of the run)\n",
    "# Output\n",
    "compile=False # whether to torch.compile the model for speed (may increase memory usage)\n",
    "model_tag = \"\" # optionally override the model tag for the output checkpoint directory name\n",
    "# now allow CLI to override the settings via the configurator lol\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "exec(open('reqchat/configurator.py').read(), globals()) # overrides from command line or config file\n",
    "user_config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "991bb95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 22:07:46,327 - reqchat.common - \u001b[32m\u001b[1mINFO\u001b[0m - Distributed world size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cpu\n"
     ]
    }
   ],
   "source": [
    "# Compute init\n",
    "device_type = autodetect_device_type() if device_type == \"\" else device_type\n",
    "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n",
    "master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()\n",
    "synchronize = torch.cuda.synchronize if device_type == \"cuda\" else lambda: None\n",
    "get_max_memory = torch.cuda.max_memory_allocated if device_type == \"cuda\" else lambda: 0\n",
    "\n",
    "# wandb logging init\n",
    "use_dummy_wandb = run == \"dummy\" or not master_process\n",
    "wandb_run = DummyWandb() if use_dummy_wandb else wandb.init(project=\"reqchat\", name=run, config=user_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81a60750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 65,536\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer will be useful for evaluation, also we need the vocab size\n",
    "tokenizer = get_tokenizer()\n",
    "token_bytes = get_token_bytes(device=device)\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print0(f\"Vocab size: {vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd1c2fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers: 20\n",
      "model_dim: 1280\n",
      "num_heads: 10\n",
      "num_kv_heads: 10\n"
     ]
    }
   ],
   "source": [
    "# Model kwargs are derived from the desired depth of the model\n",
    "num_layers = depth\n",
    "model_dim = depth * 64 # aspect ratio 64 (usually this is varied from 64 -> 128 as model size increases)\n",
    "num_heads = max(1, (model_dim + 127) // 128) # head dim 128 (the division here is ceil div)\n",
    "num_kv_heads = num_heads # default is 1:1 GQA (Group Query Attention) ratio (i.e. GQA is disabled)\n",
    "print0(f\"num_layers: {num_layers}\")\n",
    "print0(f\"model_dim: {model_dim}\")\n",
    "print0(f\"num_heads: {num_heads}\")\n",
    "print0(f\"num_kv_heads: {num_kv_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22996f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens / micro-batch / rank: 1 x 64 = 64\n",
      "Tokens / micro-batch: 64\n",
      "Total batch size 64 => gradient accumulation steps: 1\n"
     ]
    }
   ],
   "source": [
    "# Optimizer / data / training length related hyperparameters\n",
    "# figure out the needed gradient accumulation to reach the desired total batch size\n",
    "tokens_per_fwdbwd = device_batch_size * max_seq_len # tokens per iteration for a single rank\n",
    "world_tokens_per_fwdbwd = tokens_per_fwdbwd * ddp_world_size # total tokens per iteration for all ranks\n",
    "assert total_batch_size % world_tokens_per_fwdbwd == 0\n",
    "grad_accum_steps = total_batch_size // world_tokens_per_fwdbwd\n",
    "print0(f\"Tokens / micro-batch / rank: {device_batch_size} x {max_seq_len} = {tokens_per_fwdbwd:,}\")\n",
    "print0(f\"Tokens / micro-batch: {world_tokens_per_fwdbwd:,}\")\n",
    "print0(f\"Total batch size {total_batch_size:,} => gradient accumulation steps: {grad_accum_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df8b0c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Initialize the Model\n",
    "\n",
    "# Create a new model with random weights\n",
    "model_config_kwargs = dict(sequence_len=max_seq_len, vocab_size=vocab_size, n_layer=num_layers, n_head=num_heads, n_kv_head=num_kv_heads, n_embd=model_dim)\n",
    "with torch.device(\"meta\"):\n",
    "    model_config = GPTConfig(**model_config_kwargs)\n",
    "    model = GPT(model_config)\n",
    "model.to_empty(device=device)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e3478e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are resuming, overwrite the model parameters with those of the checkpoint\n",
    "base_dir = get_base_dir()\n",
    "output_dirname = model_tag if model_tag else f\"d{depth}\" # e.g. d12\n",
    "checkpoint_dir = os.path.join(base_dir, \"base_checkpoints\", output_dirname)\n",
    "resuming = resume_from_step != -1\n",
    "if resuming:\n",
    "    print0(f\"Resuming optimization from step {resume_from_step}\")\n",
    "    model_data, optimizer_data, meta_data = load_checkpoint(checkpoint_dir, resume_from_step, device, load_optimizer=True, rank=ddp_rank)\n",
    "    model.load_state_dict(model_data, strict=True, assign=True)\n",
    "    del model_data # free up this memory after the copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46224f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 560,988,160\n",
      "Estimated FLOPs per token: 2.882273e+09\n"
     ]
    }
   ],
   "source": [
    "orig_model = model # original, uncompiled model, for saving raw model state_dict and for inference/evaluation (because the shapes may change shape)\n",
    "model = torch.compile(model, dynamic=False) # the inputs to model will never change shape so dynamic=False is safe\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print0(f\"Number of parameters: {num_params:,}\")\n",
    "num_flops_per_token = model.estimate_flops()\n",
    "print0(f\"Estimated FLOPs per token: {num_flops_per_token:e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12b4fc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using user-provided number of iterations: 20\n",
      "Total number of training tokens: 1,280\n",
      "Tokens : Params ratio: 0.00\n",
      "Total training FLOPs estimate: 3.689310e+12\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of iterations. Either it is given, or from target flops, or from target data:param ratio (in that order)\n",
    "assert num_iterations > 0 or target_param_data_ratio > 0 or target_flops > 0\n",
    "if num_iterations > 0:\n",
    "    print0(f\"Using user-provided number of iterations: {num_iterations:,}\")\n",
    "elif target_flops > 0:\n",
    "    # calculate the number of iterations from the target flops\n",
    "    num_iterations = round(target_flops / (num_flops_per_token * total_batch_size))\n",
    "    print0(f\"Calculated number of iterations from target FLOPs: {num_iterations:,}\")\n",
    "elif target_param_data_ratio > 0:\n",
    "    # calculate the number of iterations from the target param data ratio\n",
    "    target_tokens = target_param_data_ratio * num_params\n",
    "    num_iterations = target_tokens // total_batch_size\n",
    "    print0(f\"Calculated number of iterations from target data:param ratio: {num_iterations:,}\")\n",
    "else:\n",
    "    raise ValueError(\"No training horizon specified\")\n",
    "total_tokens = total_batch_size * num_iterations\n",
    "print0(f\"Total number of training tokens: {total_tokens:,}\")\n",
    "print0(f\"Tokens : Params ratio: {total_batch_size * num_iterations / num_params:.2f}\") # Chinchilla is ~20\n",
    "print0(f\"Total training FLOPs estimate: {num_flops_per_token * total_tokens:e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1808cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling the LR for the AdamW parameters ∝1/√(1280/768) = 0.774597\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Initialize the Optimizer (Muon for Linear layers, AdamW for embedding and lm_head)\n",
    "optimizers = model.setup_optimizers(unembedding_lr=unembedding_lr, embedding_lr=embedding_lr, matrix_lr=matrix_lr, weight_decay=weight_decay)\n",
    "adamw_optimizer, muon_optimizer = optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf27ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resuming:\n",
    "    for opt, dat in zip(optimizers, optimizer_data):\n",
    "        opt.load_state_dict(dat)\n",
    "    del optimizer_data # free up the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "886eebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Initialize the DataLoaders for train/val\n",
    "tokens_dir = os.path.join(base_dir, \"tokenized_data\")\n",
    "dataloader_resume_state_dict = None if not resuming else meta_data[\"dataloader_state_dict\"]\n",
    "train_loader = tokenizing_distributed_data_loader_with_state(device_batch_size, max_seq_len, split=\"train\", device=device, resume_state_dict=dataloader_resume_state_dict)\n",
    "build_val_loader = lambda: tokenizing_distributed_data_loader(device_batch_size, max_seq_len, split=\"val\", device=device)\n",
    "x, y, dataloader_state_dict = next(train_loader) # kick off load of the very first batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7e839c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Set up hyperparameter schedulers\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr_multiplier(it):\n",
    "    warmup_iters = round(warmup_ratio * num_iterations)\n",
    "    warmdown_iters = round(warmdown_ratio * num_iterations)\n",
    "    if it < warmup_iters:\n",
    "        return (it + 1) / warmup_iters\n",
    "    elif it <= num_iterations - warmdown_iters:\n",
    "        return 1.0\n",
    "    else:\n",
    "        progress = (num_iterations - it) / warmdown_iters\n",
    "        return progress * 1.0 + (1 - progress) * final_lr_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47b168b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum scheduler for Muon optimizer\n",
    "def get_muon_momentum(it):\n",
    "    frac = min(it / 300, 1)\n",
    "    momentum = (1 - frac) * 0.85 + frac * 0.95\n",
    "    return momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "035bc4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Loop state (variables updated by the training loop)\n",
    "\n",
    "if not resuming:\n",
    "    step = 0\n",
    "    min_val_bpb = float(\"inf\")\n",
    "    smooth_train_loss = 0 # EMA of training loss\n",
    "    total_training_time = 0 # total wall-clock time of training\n",
    "else:\n",
    "    step = meta_data[\"step\"]\n",
    "    loop_state = meta_data[\"loop_state\"]\n",
    "    min_val_bpb = loop_state[\"min_val_bpb\"]\n",
    "    smooth_train_loss = loop_state[\"smooth_train_loss\"]\n",
    "    total_training_time = loop_state[\"total_training_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "457cbff5",
   "metadata": {},
   "outputs": [
    {
     "ename": "InductorError",
     "evalue": "RuntimeError: Compiler: cl is not found.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInductorError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m eval_steps = eval_tokens // (device_batch_size * max_seq_len * ddp_world_size)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m autocast_ctx:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     val_bpb = \u001b[43mevaluate_bpb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m print0(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Validation bpb: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_bpb\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val_bpb < min_val_bpb:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\reqchat\\loss_eval.py:33\u001b[39m, in \u001b[36mevaluate_bpb\u001b[39m\u001b[34m(model, batches, steps, token_bytes)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[32m     32\u001b[39m     x, y = \u001b[38;5;28mnext\u001b[39m(batch_iter)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     loss2d = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_reduction\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T)\u001b[39;00m\n\u001b[32m     34\u001b[39m     loss2d = loss2d.view(-\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# flatten\u001b[39;00m\n\u001b[32m     35\u001b[39m     y = y.view(-\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# flatten\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:414\u001b[39m, in \u001b[36mOptimizedModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.nn.modules.module._has_any_global_hook():\n\u001b[32m    405\u001b[39m     warnings.warn(\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `torch.compile(module)` when there are global hooks on \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmodules (e.g., from `register_module_forward_hook`); this will\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    412\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    413\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:845\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    841\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__cause__\u001b[39;00m  \u001b[38;5;66;03m# User compiler error\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    843\u001b[39m     \u001b[38;5;66;03m# Failures in the backend likely don't have useful\u001b[39;00m\n\u001b[32m    844\u001b[39m     \u001b[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m845\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.remove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001b[39;00m\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    847\u001b[39m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[32m    848\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:990\u001b[39m, in \u001b[36m_compile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **graph_kwargs)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m990\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InductorError(e, currentframe()).with_traceback(\n\u001b[32m    991\u001b[39m         e.__traceback__\n\u001b[32m    992\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    993\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    994\u001b[39m     TritonBundler.end_compile()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:974\u001b[39m, in \u001b[36m_compile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **graph_kwargs)\u001b[39m\n\u001b[32m    972\u001b[39m TritonBundler.begin_compile()\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m     mb_compiled_graph = \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgraph_kwargs\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m mb_compiled_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    978\u001b[39m     mb_compiled_graph._time_taken_ns = time.time_ns() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1695\u001b[39m, in \u001b[36mfx_codegen_and_compile\u001b[39m\u001b[34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001b[39m\n\u001b[32m   1691\u001b[39m     fast_scheme = _InProcessFxCompile()\n\u001b[32m   1693\u001b[39m     scheme = _ProgressiveFxCompile(fast_scheme, scheme, progression_configs)\n\u001b[32m-> \u001b[39m\u001b[32m1695\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheme\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1505\u001b[39m, in \u001b[36m_InProcessFxCompile.codegen_and_compile\u001b[39m\u001b[34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001b[39m\n\u001b[32m   1487\u001b[39m         compiled_fn = AotCodeCompiler.compile(\n\u001b[32m   1488\u001b[39m             graph,\n\u001b[32m   1489\u001b[39m             wrapper_code.value,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1502\u001b[39m             ],\n\u001b[32m   1503\u001b[39m         )\n\u001b[32m   1504\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1505\u001b[39m     compiled_module = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m     compiled_fn = compiled_module.call\n\u001b[32m   1507\u001b[39m     compiled_fn_runner = \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   1508\u001b[39m         compiled_module, \u001b[33m\"\u001b[39m\u001b[33mrunner\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1509\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\graph.py:2319\u001b[39m, in \u001b[36mGraphLowering.compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CompiledModule:\n\u001b[32m   2313\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m   2314\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGraphLowering.compile_to_module\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2315\u001b[39m         phase_name=\u001b[33m\"\u001b[39m\u001b[33mcode_gen\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2316\u001b[39m         log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   2317\u001b[39m         dynamo_compile_column_us=\u001b[33m\"\u001b[39m\u001b[33minductor_code_gen_cumulative_compile_time_us\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2318\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m2319\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\graph.py:2325\u001b[39m, in \u001b[36mGraphLowering._compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2321\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CompiledModule:\n\u001b[32m   2322\u001b[39m     \u001b[38;5;66;03m# If we're here, we don't have to worry about the kernel code, which is only\u001b[39;00m\n\u001b[32m   2323\u001b[39m     \u001b[38;5;66;03m# returned separately in AOTInductor mode.\u001b[39;00m\n\u001b[32m   2324\u001b[39m     wrapper_code, _ = (\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m         \u001b[38;5;28mself\u001b[39m.codegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2326\u001b[39m     )\n\u001b[32m   2328\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wrapper_code, ValueWithLineMap):\n\u001b[32m   2329\u001b[39m         mod = \u001b[38;5;28mself\u001b[39m._compile_to_module_lines(wrapper_code)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\graph.py:2264\u001b[39m, in \u001b[36mGraphLowering.codegen\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2261\u001b[39m V.debug.draw_orig_fx_graph(\u001b[38;5;28mself\u001b[39m.orig_gm, \u001b[38;5;28mself\u001b[39m.scheduler.nodes)\n\u001b[32m   2263\u001b[39m \u001b[38;5;28mself\u001b[39m.wrapper_code.push_codegened_graph(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2264\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2266\u001b[39m log.debug(\n\u001b[32m   2267\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFinished codegen for all nodes. The list of kernel names available: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   2268\u001b[39m     V.graph.all_codegen_kernel_names,\n\u001b[32m   2269\u001b[39m )\n\u001b[32m   2271\u001b[39m result = \u001b[38;5;28mself\u001b[39m.wrapper_code.generate(\u001b[38;5;28mself\u001b[39m.is_inference)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:5197\u001b[39m, in \u001b[36mScheduler.codegen\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   5194\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcodegen\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5195\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33mScheduler.codegen\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   5196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m5197\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_codegen_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5198\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m torch._inductor.config.graph_partition\n\u001b[32m   5199\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._codegen(\u001b[38;5;28mself\u001b[39m.nodes)\n\u001b[32m   5200\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:5337\u001b[39m, in \u001b[36mScheduler._codegen_partitions\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   5332\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(partition) >= \u001b[32m1\u001b[39m, (\n\u001b[32m   5333\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEach partition must have at least one node but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(partition)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   5334\u001b[39m )\n\u001b[32m   5336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m signature.skip_cudagraph:\n\u001b[32m-> \u001b[39m\u001b[32m5337\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_codegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5339\u001b[39m     \u001b[38;5;28mself\u001b[39m._codegen_partition_wrapper(partition, signature)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:5435\u001b[39m, in \u001b[36mScheduler._codegen\u001b[39m\u001b[34m(self, nodes)\u001b[39m\n\u001b[32m   5433\u001b[39m     backend.codegen_combo_kernel(node)\n\u001b[32m   5434\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, (FusedSchedulerNode, SchedulerNode)):\n\u001b[32m-> \u001b[39m\u001b[32m5435\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5436\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5437\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, NopKernelSchedulerNode)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:5278\u001b[39m, in \u001b[36mCppScheduling.codegen_node\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m   5275\u001b[39m kernel_group = \u001b[38;5;28mself\u001b[39m.kernel_group\n\u001b[32m   5277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, OuterLoopFusedSchedulerNode):\n\u001b[32m-> \u001b[39m\u001b[32m5278\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcodegen_outer_loop_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5279\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5280\u001b[39m     nodes: \u001b[38;5;28mlist\u001b[39m[SchedulerNode] = node.get_nodes()  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:5252\u001b[39m, in \u001b[36mCppScheduling.codegen_outer_loop_node\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m   5245\u001b[39m         kernel_group.finalize_kernel(\n\u001b[32m   5246\u001b[39m             outer_fusion_cpp_kernel_proxy,\n\u001b[32m   5247\u001b[39m             [*itertools.chain.from_iterable(nodes_list)],\n\u001b[32m   5248\u001b[39m         )\n\u001b[32m   5250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtry_outer_loop_fusion_with_local_buf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   5253\u001b[39m     \u001b[38;5;66;03m# Reset generated_cpp_vec_kernel_count to codegen again\u001b[39;00m\n\u001b[32m   5254\u001b[39m     metrics.generated_cpp_vec_kernel_count = generated_cpp_vec_kernel_count\n\u001b[32m   5255\u001b[39m     cpp_kernel_proxy_list.clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:5223\u001b[39m, in \u001b[36mCppScheduling.codegen_outer_loop_node.<locals>.try_outer_loop_fusion_with_local_buf\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m   5221\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _node \u001b[38;5;129;01min\u001b[39;00m node.get_outer_nodes():\n\u001b[32m   5222\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_node, (FusedSchedulerNode, SchedulerNode))\n\u001b[32m-> \u001b[39m\u001b[32m5223\u001b[39m     cpp_kernel_proxy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel_proxy_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5224\u001b[39m     cpp_kernel_proxy.codegen_nodes(_node.get_nodes())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   5225\u001b[39m     cpp_kernel_proxy_list.append(cpp_kernel_proxy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:4008\u001b[39m, in \u001b[36mCppKernelProxy.__init__\u001b[39m\u001b[34m(self, kernel_group)\u001b[39m\n\u001b[32m   4006\u001b[39m \u001b[38;5;28mself\u001b[39m.loop_nest = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4007\u001b[39m \u001b[38;5;28mself\u001b[39m.call_ranges = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4008\u001b[39m \u001b[38;5;28mself\u001b[39m.picked_vec_isa: cpu_vec_isa.VecISA = \u001b[43mcpu_vec_isa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpick_vec_isa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4009\u001b[39m \u001b[38;5;28mself\u001b[39m.kernels: \u001b[38;5;28mlist\u001b[39m[CppKernel] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:497\u001b[39m, in \u001b[36mpick_vec_isa\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.is_fbcode() \u001b[38;5;129;01mand\u001b[39;00m (platform.machine() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mx86_64\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAMD64\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m    495\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m VecAVX2()\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m _valid_vec_isa_list: \u001b[38;5;28mlist\u001b[39m[VecISA] = \u001b[43mvalid_vec_isa_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _valid_vec_isa_list:\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_vec_isa\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:484\u001b[39m, in \u001b[36mvalid_vec_isa_list\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[33;03m    arch value is x86_64 on Linux, and the value is AMD64 on Windows.\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    483\u001b[39m     _cpu_supported_x86_isa = x86_isa_checker()\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     \u001b[43misa_list\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43misa\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msupported_vec_isa_list\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mflag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_cpu_supported_x86_isa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43misa\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m isa_list\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:484\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[33;03m    arch value is x86_64 on Linux, and the value is AMD64 on Windows.\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    483\u001b[39m     _cpu_supported_x86_isa = x86_isa_checker()\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     isa_list.extend(\n\u001b[32m    485\u001b[39m         isa\n\u001b[32m    486\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m isa \u001b[38;5;129;01min\u001b[39;00m supported_vec_isa_list\n\u001b[32m    487\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(flag \u001b[38;5;129;01min\u001b[39;00m _cpu_supported_x86_isa \u001b[38;5;28;01mfor\u001b[39;00m flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(isa).split()) \u001b[38;5;129;01mand\u001b[39;00m isa\n\u001b[32m    488\u001b[39m     )\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m isa_list\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:221\u001b[39m, in \u001b[36mVecAVX512.__bool__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;129m@functools\u001b[39m.cache  \u001b[38;5;66;03m# noqa: B019\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__bool__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    222\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m config.is_fbcode():\n\u001b[32m    223\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:143\u001b[39m, in \u001b[36mVecISA.__bool__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__bool__impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvec_isa_ok\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:153\u001b[39m, in \u001b[36mVecISA.__bool__impl\u001b[39m\u001b[34m(self, vec_isa_ok)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.is_fbcode():\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVecISA\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_avx_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:103\u001b[39m, in \u001b[36mVecISA.check_build\u001b[39m\u001b[34m(self, code)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcodecache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lock_dir, LOCK_TIMEOUT, write\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcpp_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     95\u001b[39m     CppBuilder,\n\u001b[32m     96\u001b[39m     CppTorchOptions,\n\u001b[32m     97\u001b[39m     normalize_path_separator,\n\u001b[32m     98\u001b[39m )\n\u001b[32m    100\u001b[39m key, input_path = write(\n\u001b[32m    101\u001b[39m     code,\n\u001b[32m    102\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcpp\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     extra=\u001b[43m_get_isa_dry_compile_fingerprint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_arch_flags\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    104\u001b[39m )\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_filelock\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileLock\n\u001b[32m    107\u001b[39m lock_dir = get_lock_dir()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:29\u001b[39m, in \u001b[36m_get_isa_dry_compile_fingerprint\u001b[39m\u001b[34m(isa_flags)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_isa_dry_compile_fingerprint\u001b[39m(isa_flags: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# ISA dry compile will cost about 1 sec time each startup time.\u001b[39;00m\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# Please check the issue: https://github.com/pytorch/pytorch/issues/100378\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# and generated them to output binary hash path.\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# It would optimize and skip compile existing binary.\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcpp_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compiler_version_info, get_cpp_compiler\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     compiler_info = get_compiler_version_info(\u001b[43mget_cpp_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     30\u001b[39m     torch_version = torch.__version__\n\u001b[32m     31\u001b[39m     fingerprint = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompiler_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00misa_flags\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py:338\u001b[39m, in \u001b[36mget_cpp_compiler\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    336\u001b[39m     compiler = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mCXX\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    337\u001b[39m     compiler = normalize_path_separator(compiler)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     \u001b[43mcheck_compiler_exist_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     check_msvc_cl_language_id(compiler)\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohan\\Documents\\Forward_plan\\misc\\Applying_process\\Daily_AI\\Day_10\\reqchat\\venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py:139\u001b[39m, in \u001b[36mcheck_compiler_exist_windows\u001b[39m\u001b[34m(compiler)\u001b[39m\n\u001b[32m    137\u001b[39m     subprocess.check_output([compiler, \u001b[33m\"\u001b[39m\u001b[33m/help\u001b[39m\u001b[33m\"\u001b[39m], stderr=subprocess.STDOUT)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCompiler: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompiler\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not found.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess.SubprocessError:\n\u001b[32m    141\u001b[39m     \u001b[38;5;66;03m# Expected that some compiler(clang, clang++) is exist, but they not support `/help` args.\u001b[39;00m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mInductorError\u001b[39m: RuntimeError: Compiler: cl is not found.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Training loop\n",
    "while True:\n",
    "    last_step = step == num_iterations # loop runs num_iterations+1 times so that we can eval/save at the end\n",
    "    flops_so_far = num_flops_per_token * total_batch_size * step\n",
    "\n",
    "    # once in a while: evaluate the val bpb (all ranks participate)\n",
    "    if last_step or step % eval_every == 0:\n",
    "        model.eval()\n",
    "        val_loader = build_val_loader()\n",
    "        eval_steps = eval_tokens // (device_batch_size * max_seq_len * ddp_world_size)\n",
    "        with autocast_ctx:\n",
    "            val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)\n",
    "        print0(f\"Step {step:05d} | Validation bpb: {val_bpb:.4f}\")\n",
    "        if val_bpb < min_val_bpb:\n",
    "            min_val_bpb = val_bpb\n",
    "        wandb_run.log({\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"total_training_time\": total_training_time,\n",
    "            \"val/bpb\": val_bpb,\n",
    "        })\n",
    "        model.train()\n",
    "\n",
    "    # once in a while: estimate the CORE metric (all ranks participate)\n",
    "    # use the original uncompiled model because the inputs keep changing shape\n",
    "    results = {}\n",
    "    if core_metric_every > 0 and (last_step or (step > 0 and step % core_metric_every == 0)):\n",
    "        model.eval()\n",
    "        with autocast_ctx:\n",
    "            results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
    "        print0(f\"Step {step:05d} | CORE metric: {results['core_metric']:.4f}\")\n",
    "        wandb_run.log({\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"core_metric\": results[\"core_metric\"],\n",
    "            \"centered_results\": results[\"centered_results\"],\n",
    "        })\n",
    "        model.train()\n",
    "\n",
    "    # once in a while: sample from the model (only on master process)\n",
    "    # use the original uncompiled model because the inputs keep changing shape\n",
    "    if master_process and (last_step or (step > 0 and step % sample_every == 0)):\n",
    "        model.eval()\n",
    "        prompts = [\n",
    "            \"The capital of France is\",\n",
    "            \"The chemical symbol of gold is\",\n",
    "            \"If yesterday was Friday, then tomorrow will be\",\n",
    "            \"The opposite of hot is\",\n",
    "            \"The planets of the solar system are:\",\n",
    "            \"My favorite color is\",\n",
    "            \"If 5*x + 3 = 13, then x is\",\n",
    "        ]\n",
    "        engine = Engine(orig_model, tokenizer) # use orig_model to avoid recompilation\n",
    "        for prompt in prompts:\n",
    "            tokens = tokenizer(prompt, prepend=\"<|bos|>\")\n",
    "            with autocast_ctx:\n",
    "                sample, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=16, temperature=0)\n",
    "            print0(tokenizer.decode(sample[0]))\n",
    "        model.train()\n",
    "\n",
    "    # save checkpoint: at the end of the run, or every save_every steps, except at the first step or the resume step\n",
    "    if last_step or (step > 0 and step != resume_from_step and save_every > 0 and step % save_every == 0):\n",
    "        save_checkpoint(\n",
    "            checkpoint_dir,\n",
    "            step,\n",
    "            orig_model.state_dict(), # model parameters\n",
    "            [opt.state_dict() for opt in optimizers], # optimizer states\n",
    "            { # metadata saved as json\n",
    "                \"step\": step,\n",
    "                \"val_bpb\": val_bpb, # loss at last step\n",
    "                \"model_config\": model_config_kwargs,\n",
    "                \"user_config\": user_config, # inputs to the training script\n",
    "                \"device_batch_size\": device_batch_size,\n",
    "                \"max_seq_len\": max_seq_len,\n",
    "                \"dataloader_state_dict\": dataloader_state_dict,\n",
    "                \"loop_state\": { # all loop state (other than step) so that we can resume training\n",
    "                    \"min_val_bpb\": min_val_bpb,\n",
    "                    \"smooth_train_loss\": smooth_train_loss,\n",
    "                    \"total_training_time\": total_training_time,\n",
    "                },\n",
    "            },\n",
    "            rank=ddp_rank,\n",
    "        )\n",
    "\n",
    "    # termination conditions (TODO: possibly also add loss explosions etc.)\n",
    "    if last_step:\n",
    "        break\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # single training step\n",
    "    # evaluate the gradient\n",
    "    synchronize()\n",
    "    t0 = time.time()\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        with autocast_ctx:\n",
    "            loss = model(x, y)\n",
    "        train_loss = loss.detach() # for logging\n",
    "        loss = loss / grad_accum_steps # each .backward() is a grad sum => normalize loss here\n",
    "        loss.backward()\n",
    "        x, y, dataloader_state_dict = next(train_loader) # prefetch the next batch while the GPU is busy with forward/backward\n",
    "    # gradient clipping\n",
    "    grad_clip_enabled = grad_clip > 0.0\n",
    "    if grad_clip_enabled:\n",
    "        grad_norm_tensor = torch.nn.utils.clip_grad_norm_(orig_model.parameters(), grad_clip)\n",
    "        grad_norm = grad_norm_tensor.item() # GPU tensor -> CPU float (note: cpu-gpu sync point)\n",
    "    # step the optimizers\n",
    "    lrm = get_lr_multiplier(step)\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = group[\"initial_lr\"] * lrm\n",
    "    muon_momentum = get_muon_momentum(step)\n",
    "    for group in muon_optimizer.param_groups:\n",
    "        group[\"momentum\"] = muon_momentum\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # logging\n",
    "    ema_beta = 0.9 # EMA decay factor for some smoothing just for nicer logging\n",
    "    smooth_train_loss = ema_beta * smooth_train_loss + (1 - ema_beta) * train_loss.item() # EMA the training loss\n",
    "    debiased_smooth_loss = smooth_train_loss / (1 - ema_beta**(step + 1)) # debias the EMA\n",
    "    pct_done = 100 * step / num_iterations\n",
    "    tok_per_sec = int(total_batch_size / dt)\n",
    "    flops_per_sec = num_flops_per_token * total_batch_size / dt\n",
    "    promised_flops_per_sec_h100 = 989e12 * ddp_world_size # bfloat16 H100 SXM and without 2:4 sparsity\n",
    "    mfu = 100 * flops_per_sec / promised_flops_per_sec_h100 # in %\n",
    "    if step > 10:\n",
    "        total_training_time += dt # only count the time after the first 10 steps\n",
    "    print_grad_norm = f\" grad norm: {grad_norm:.4f} |\" if grad_clip_enabled else \"\"\n",
    "    print0(f\"step {step:05d}/{num_iterations:05d} ({pct_done:.2f}%) | loss: {debiased_smooth_loss:.6f} |{print_grad_norm} lrm: {lrm:.2f} | dt: {dt * 1000:.2f}ms | tok/sec: {tok_per_sec:,} | mfu: {mfu:.2f} | total time: {total_training_time/60:.2f}m\")\n",
    "    if step % 100 == 0:\n",
    "        log_data = {\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"total_training_time\": total_training_time,\n",
    "            \"train/loss\": debiased_smooth_loss,\n",
    "            \"train/lrm\": lrm,\n",
    "            \"train/dt\": dt,\n",
    "            \"train/tok_per_sec\": tok_per_sec,\n",
    "            \"train/mfu\": mfu,\n",
    "        }\n",
    "        if grad_clip_enabled:\n",
    "            log_data[\"train/grad_norm\"] = grad_norm\n",
    "        wandb_run.log(log_data)\n",
    "\n",
    "    # state update\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6402b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a few more stats\n",
    "print0(f\"Peak memory usage: {get_max_memory() / 1024 / 1024:.2f}MiB\")\n",
    "print0(f\"Total training time: {total_training_time/60:.2f}m\")\n",
    "print0(f\"Minimum validation bpb: {min_val_bpb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae454cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log to report\n",
    "from reqchat.report import get_report\n",
    "get_report().log(section=\"Base model training\", data=[\n",
    "    user_config, # CLI args\n",
    "    { # stats about the training setup\n",
    "        \"Number of parameters\": num_params,\n",
    "        \"Number of FLOPs per token\": f\"{num_flops_per_token:e}\",\n",
    "        \"Calculated number of iterations\": num_iterations,\n",
    "        \"Number of training tokens\": total_tokens,\n",
    "        \"Tokens : Params ratio\": total_batch_size * num_iterations / num_params,\n",
    "        \"DDP world size\": ddp_world_size,\n",
    "        \"warmup_ratio\": warmup_ratio,\n",
    "        \"warmdown_ratio\": warmdown_ratio,\n",
    "        \"final_lr_frac\": final_lr_frac,\n",
    "    },\n",
    "    { # stats about training outcomes\n",
    "        \"Minimum validation bpb\": min_val_bpb,\n",
    "        \"Final validation bpb\": val_bpb,\n",
    "        \"CORE metric estimate\": results.get(\"core_metric\", None),\n",
    "        \"MFU %\": f\"{mfu:.2f}%\",\n",
    "        \"Total training flops\": f\"{flops_so_far:e}\",\n",
    "        \"Total training time\": f\"{total_training_time/60:.2f}m\",\n",
    "        \"Peak memory usage\": f\"{get_max_memory() / 1024 / 1024:.2f}MiB\",\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a4841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "wandb_run.finish() # wandb run finish\n",
    "compute_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1271f5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95e321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
